{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Misc imports\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert string to one hot encoding\n",
    "def to_onehot(data):\n",
    "    ascii_list = []\n",
    "    elements_per_song = []\n",
    "    for songs in data:\n",
    "        elements_per_song.append(len(songs))\n",
    "        for chars in songs:\n",
    "            ascii_list.append(ord(chars))\n",
    "    onehot_vals = np.eye(128)[ascii_list]\n",
    "    return onehot_vals, np.array(ascii_list), np.array(elements_per_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is a list of list of chars, each string list is a song\n",
    "#onehot_ascii output are shifted up by 1, need to figure out what to do with 0 array labels\n",
    "def onehot_songs(data, sequence_length):\n",
    "    max_num_chars = 0\n",
    "    temp_list = []\n",
    "    onehot_ascii = []\n",
    "    onehot_songs = []\n",
    "    \n",
    "    #List of songs in ascii format\n",
    "    for song in data:\n",
    "        if len(song)>max_num_chars:\n",
    "            max_num_chars = len(song) #Finds longest song length\n",
    "        for chars in song:\n",
    "            temp_list.append(ord(chars))\n",
    "        onehot_ascii.append(temp_list)\n",
    "        temp_list = []\n",
    "        \n",
    "    #To make divisible by sequence_length per batch\n",
    "    max_num_chars = sequence_length-(max_num_chars%sequence_length)+max_num_chars\n",
    "    codding = np.append(np.zeros((1,128)),np.eye(128),0)    \n",
    "    \n",
    "    for i, ascii_song in enumerate(onehot_ascii):\n",
    "        ascii_song = np.array(ascii_song)+1 #since making first row of eye matrix all 0s\n",
    "        needed_0s = max_num_chars-len(ascii_song)\n",
    "        onehot_ascii[i] = np.pad(ascii_song,(0,needed_0s),'constant',constant_values=0)\n",
    "        onehot_songs.append(codding[onehot_ascii[i]])\n",
    "        \n",
    "    return onehot_ascii, onehot_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is onehot array of every char in train\n",
    "#labels are every ascii value of every char in train\n",
    "def make_sequence(data,labels,sequence_length):\n",
    "    num_groups = np.floor(len(labels)/sequence_length) #length labels same as data\n",
    "    remainder = len(labels)%sequence_length\n",
    "    #Add 1 here to grab one time step ahead for predicting labels\n",
    "    label_groups = np.split(labels[0+1:len(labels)-remainder+1],num_groups)\n",
    "    onehot_groups = np.vsplit(data[0:len(labels)-remainder],num_groups)\n",
    "    return np.array(onehot_groups),np.array(label_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size is the number of sequences per batch\n",
    "def batching(onehot_data,labels,batch_size):\n",
    "    num_batches = np.floor(len(labels)/batch_size)\n",
    "    num_subtract = len(labels)%batch_size\n",
    "    label_batches = np.vsplit(labels[0:len(labels)-num_subtract],num_batches)\n",
    "    onehot_batches = np.split(onehot_data[0:len(labels)-num_subtract],num_batches,0)\n",
    "    return np.array(onehot_batches),np.array(label_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Class LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rnn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=1,batch_size=1,\n",
    "                 sequence_length=25,batch_first = True):\n",
    "        super(lstm_rnn, self).__init__()\n",
    "        #Constants\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        #Initializing model\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first = batch_first)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "    def forward(self,data,initial):\n",
    "        #data = data.view(self.batch_size, self.sequence_length, self.input_size)\n",
    "        output,hidden = self.lstm(data,initial)\n",
    "        out = self.fc(output.contiguous().view(-1,output.size(2)))\n",
    "        #The softmax reshapes it back to what the dimensions of the output of lstm are\n",
    "        #out = F.softmax(output)\n",
    "        return out,hidden\n",
    "        \n",
    "    def init_hidden(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_hidden = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_hidden = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_hidden\n",
    "    \n",
    "    def init_cell(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_cell = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_cell = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/input.txt\")\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "text_array = np.asarray(text)\n",
    "\n",
    "#Creates list of each song\n",
    "indicies = np.where(text_array == '<start>\\n') #Location of where each abc file starts\n",
    "data = []\n",
    "for i in range(len(indicies[0])):\n",
    "    if i+1 == len(indicies[0]):\n",
    "        #For the last abc file\n",
    "        abc = text_array[indicies[0][i]:]\n",
    "    else:\n",
    "        abc = text_array[indicies[0][i]:indicies[0][i+1]]\n",
    "    data.append(''.join(abc))\n",
    "\n",
    "#print(data[0])\n",
    "#print(data[1123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80 - 20 split on data -> training and validation\n",
    "#Constants\n",
    "train_len = int(np.floor(len(data)*0.8))\n",
    "validation_len = len(data) - train_len\n",
    "\n",
    "np.random.seed(0)\n",
    "#Each index references to a single song\n",
    "indxs = np.asarray(range(len(data)))\n",
    "np.random.shuffle(indxs)\n",
    "\n",
    "train_data = (np.asarray(data))[indxs[0:train_len]]\n",
    "validation_data = (np.asarray(data))[indxs[train_len:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Data and LSTM constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM constants\n",
    "input_size = 128 #num_classes = 128\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "batch_size = 500\n",
    "sequence_length = 25\n",
    "\n",
    "#Data\n",
    "#combines all the songs and makes a single string, converted to ascii and onehot\n",
    "onehot_train_full, train_labels_full, train_NumPerSong = to_onehot(train_data)\n",
    "onehot_validation_full, validation_labels_full, validation_NumPerSong = to_onehot(validation_data)\n",
    "#print(train_labels[0])\n",
    "#print(onehot_train[0])\n",
    "\n",
    "#or using onehot song encoddings\n",
    "#onehot_train, train_labels_padded = onehot_songs(train_data,sequence_length)\n",
    "#onehot_validation, validation_labels_padded = onehot_songs(validation_data,sequence_length)\n",
    "\n",
    "#divide data based on sequence length\n",
    "onehot_train, train_labels = make_sequence(onehot_train_full,\n",
    "                                           train_labels_full,\n",
    "                                           sequence_length)\n",
    "\n",
    "onehot_validation, validation_labels = make_sequence(onehot_validation_full,\n",
    "                                                     validation_labels_full,\n",
    "                                                     sequence_length)\n",
    "\n",
    "#Makes batches, labels are 1 time step ahead for predicting outputs\n",
    "onehot_batches,batch_labels = batching(onehot_train,train_labels,batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run first in single batches, each batch has 25 chars per(sequence length 25), just run it in order of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature size:  128\n",
      "Number of hidden LSTM units:  128\n",
      "Batch Size:  500\n",
      "Elements per sequence:  25\n",
      "Number of LSTM layers:  1\n",
      "\n",
      " lstm_rnn(\n",
      "  (lstm): LSTM(128, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=128)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm = lstm_rnn(input_size,hidden_size,num_layers,batch_size,\n",
    "                sequence_length,batch_first = batch_first)\n",
    "lstm.cuda()\n",
    "\n",
    "print(\"Input feature size: \", lstm.input_size)\n",
    "print(\"Number of hidden LSTM units: \", lstm.hidden_size)\n",
    "print(\"Batch Size: \", lstm.batch_size)\n",
    "print(\"Elements per sequence: \", lstm.sequence_length)\n",
    "print(\"Number of LSTM layers: \", lstm.num_layers)\n",
    "print(\"\\n\",lstm)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "#Initialize hidden states\n",
    "h_0 = lstm.init_hidden().cuda()\n",
    "c_0 = lstm.init_cell().cuda()\n",
    "states = (h_0,c_0)\n",
    "loss = []\n",
    "accuracy = []\n",
    "num_batches = len(onehot_batches) #when using 500 per batch, results in 32 total batches\n",
    "    \n",
    "for epoch in range(500):\n",
    "    for indx in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        in_data = autograd.Variable(torch.FloatTensor(onehot_batches[indx])).cuda()\n",
    "        output,states = lstm(in_data,states)\n",
    "        \n",
    "        h_0 = autograd.Variable(states[0].data, requires_grad=True).cuda()\n",
    "        c_0 = autograd.Variable(states[1].data, requires_grad=True).cuda()\n",
    "        states = (h_0,c_0)\n",
    "        \n",
    "        #Labels are 1 time step ahead in time\n",
    "        #500 by 25 per batch\n",
    "        labels = batch_labels[indx].reshape(batch_size*sequence_length)\n",
    "        labels = autograd.Variable(torch.LongTensor(labels)).cuda() \n",
    "                            \n",
    "        #pretty sure that loss function will average over number of inputs\n",
    "        #if not using linear layer\n",
    "        #output = output.contiguous().view(-1,output.size(2))\n",
    "        loss_temp = criterion(output,labels)\n",
    "        loss.append(loss_temp.data)\n",
    "        loss_temp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #vals,max_indxs = (output.data).max(2)\n",
    "        #acc_temp = (max_indxs==(labels.data)).sum()\n",
    "        #accuracy.append(acc_temp/sequence_length)\n",
    "        #print(loss)\n",
    "        #print(accuracy)\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f930e2d4358>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f936cbbd2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for this later \n",
    "primer = \"<start>\\n\"\n",
    "start_token = []\n",
    "for chars in primer:\n",
    "    start_token.append(ord(chars))\n",
    "    \n",
    "start_token = np.eye(128)[start_token]\n",
    "start_token = torch.from_numpy(start_token).float()\n",
    "start_token = autograd.Variable(start_token.view(1,8,input_size))\n",
    "#print(start_token.argmax(1))\n",
    "\n",
    "end = \"<end>\"\n",
    "end_token = []\n",
    "for chars in end:\n",
    "    end_token.append(ord(chars))\n",
    "    \n",
    "end_token = np.eye(128)[end_token]\n",
    "\n",
    "done = 1\n",
    "\n",
    "#Generating music\n",
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "chars = []\n",
    "\n",
    "#Each loop generates 1 char\n",
    "for i in range(100):\n",
    "    output,states = lstm(start_token,states)\n",
    "    \n",
    "    h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "    c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "    states = (h_0,c_0)\n",
    "    \n",
    "    #Currently I am taking the max of the hidden state as the letter that it\n",
    "    #produces, the TA on piazza said that it is not ideal to take max,\n",
    "    #can try implementing what he says later\n",
    "    val,indx = states[0].data.max(2)\n",
    "    chars.append(indx)\n",
    "    \n",
    "    start_token = torch.from_numpy(np.eye(128)[indx]).float()\n",
    "    start_token = autograd.Variable(start_token.view(1,1,input_size))\n",
    "    \n",
    "    #Need to implement a check for \"<end>\", generation should stop once\n",
    "    #\"<end>\" is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting back to strings\n",
    "outputs = []\n",
    "for letter in chars:\n",
    "    outputs.append(chr(letter))\n",
    "\n",
    "print(chars)\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
