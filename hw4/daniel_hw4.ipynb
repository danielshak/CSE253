{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Misc imports\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert string to one hot encoding\n",
    "def to_onehot(data):\n",
    "    ascii_list = []\n",
    "    for songs in data:\n",
    "        for chars in songs:\n",
    "            ascii_list.append(ord(chars))\n",
    "    onehot_vals = np.eye(128)[ascii_list]\n",
    "    return onehot_vals, np.array(ascii_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is a list of list of chars, each string list is a song\n",
    "#onehot_ascii output are shifted up by 1, need to figure out what to do with 0 array labels\n",
    "def onehot_songs(data, sequence_length):\n",
    "    max_num_chars = 0\n",
    "    temp_list = []\n",
    "    onehot_ascii = []\n",
    "    onehot_songs = []\n",
    "    \n",
    "    #List of songs in ascii format\n",
    "    for song in data:\n",
    "        if len(song)>max_num_chars:\n",
    "            max_num_chars = len(song) #Finds longest song length\n",
    "        for chars in song:\n",
    "            temp_list.append(ord(chars))\n",
    "        onehot_ascii.append(temp_list)\n",
    "        temp_list = []\n",
    "        \n",
    "    #To make divisible by sequence_length per batch\n",
    "    max_num_chars = sequence_length-(max_num_chars%sequence_length)+max_num_chars\n",
    "    codding = np.append(np.zeros((1,128)),np.eye(128),0)    \n",
    "    \n",
    "    for i, ascii_song in enumerate(onehot_ascii):\n",
    "        ascii_song = np.array(ascii_song)+1 #since making first row of eye matrix all 0s\n",
    "        needed_0s = max_num_chars-len(ascii_song)\n",
    "        onehot_ascii[i] = np.pad(ascii_song,(0,needed_0s),'constant',constant_values=0)\n",
    "        onehot_songs.append(codding[onehot_ascii[i]])\n",
    "        \n",
    "    return onehot_ascii, onehot_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/input.txt\")\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "text_array = np.asarray(text)\n",
    "\n",
    "#Creates list of each song\n",
    "indicies = np.where(text_array == '<start>\\n') #Location of where each abc file starts\n",
    "data = []\n",
    "for i in range(len(indicies[0])):\n",
    "    if i+1 == len(indicies[0]):\n",
    "        #For the last abc file\n",
    "        abc = text_array[indicies[0][i]:]\n",
    "    else:\n",
    "        abc = text_array[indicies[0][i]:indicies[0][i+1]]\n",
    "    data.append(''.join(abc))\n",
    "\n",
    "#print(data[0])\n",
    "#print(data[1123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "225\n",
      "[   0    1    2 ... 1121 1122 1123]\n",
      "[ 752  893 1050 ...  835  559  684]\n",
      "899\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "#80 - 20 split on data -> training and validation\n",
    "#Constants\n",
    "train_len = int(np.floor(len(data)*0.8))\n",
    "validation_len = len(data) - train_len\n",
    "\n",
    "print(train_len)\n",
    "print(validation_len)\n",
    "\n",
    "np.random.seed(0)\n",
    "#Each index references to a single song\n",
    "indxs = np.asarray(range(len(data)))\n",
    "print(indxs)\n",
    "np.random.shuffle(indxs)\n",
    "print(indxs)\n",
    "\n",
    "train_data = (np.asarray(data))[indxs[0:train_len]]\n",
    "validation_data = (np.asarray(data))[indxs[train_len:]]\n",
    "print(len(train_data))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97 98 99]\n",
      "<class 'numpy.ndarray'>\n",
      "[97 98 99]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Visualizing to_onehot\n",
    "b=5\n",
    "a,b = to_onehot('abc')\n",
    "print(np.argmax(a,1))\n",
    "print(type(np.argmax(a,1)))\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing onehot_songs\n",
    "train_data[0][:]\n",
    "#len(train_data[0:2])\n",
    "a,b = onehot_songs(train_data[0:2],25)\n",
    "print(b[0][374])\n",
    "print(a[0])\n",
    "print(len(a[0]))\n",
    "print(a[1])\n",
    "print(len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "#combines all the songs and makes a single string, converted to ascii and onehot\n",
    "onehot_train, train_labels = to_onehot(train_data)\n",
    "onehot_validation, validation_labels = to_onehot(validation_data)\n",
    "#print(train_labels[0])\n",
    "#print(onehot_train[0])\n",
    "\n",
    "#or using onehot song encoddings\n",
    "onehot_train, train_labels = onehot_songs(train_data)\n",
    "onehot_validation, validation_labels = onehot_songs(validation_data)\n",
    "#print(train_labels[0])\n",
    "#print(onehot_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM constants\n",
    "input_size = 128 #num_classes = 128\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "batch_size = 1\n",
    "sequence_length = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run first in single batches, each batch has 25 chars per(sequence length 25), just run it in order of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rnn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=1,batch_size=1,\n",
    "                 sequence_length=25,batch_first = True):\n",
    "        super(lstm_rnn, self).__init__()\n",
    "        #Constants\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        #Initializing model\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first = batch_first)\n",
    "        \n",
    "    def forward(self,data,initial):\n",
    "        #data = data.view(self.batch_size, self.sequence_length, self.input_size)\n",
    "        output,hidden = self.lstm(data,initial)\n",
    "        return output,hidden\n",
    "        \n",
    "    def init_hidden(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_hidden = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_hidden = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_hidden\n",
    "    \n",
    "    def init_cell(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_cell = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_cell = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm_rnn(input_size,hidden_size,num_layers,batch_size,\n",
    "                sequence_length,batch_first = batch_first)\n",
    "print(lstm.input_size)\n",
    "print(lstm.hidden_size)\n",
    "print(lstm.batch_size)\n",
    "print(lstm.sequence_length)\n",
    "print(lstm.num_layers)\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates batch indicies, +1 since arange is [,), exclusive last element\n",
    "batch_indicies = np.arange(0,len(onehot_train)-sequence_length+1,sequence_length)\n",
    "print(len(batch_indicies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "#CAN ADD LINEAR LAYER AND BATCH PROCESSING \n",
    "for epoch in range(5):\n",
    "    for indx in batch_indicies:\n",
    "        optimizer.zero_grad()\n",
    "        temp_data = onehot_train[indx:indx+25]\n",
    "        temp_data = torch.from_numpy(temp_data).float()\n",
    "        temp_data = autograd.Variable(temp_data.view(batch_size,sequence_length,input_size))\n",
    "    \n",
    "        #output is of whole sequence, states hidden is same as last element of output, also\n",
    "        #has cell hidden state\n",
    "        output, states = lstm(temp_data,states)\n",
    "        \n",
    "        h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "        c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "        states = (h_0,c_0)\n",
    "        #hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        \n",
    "        #Add 1 since we are using the next letter as the label\n",
    "        labels = autograd.Variable(torch.LongTensor(train_labels[indx+1:indx+25+1]))\n",
    "        \n",
    "        #Later try adding a linear layer here\n",
    "        \n",
    "        loss_temp = criterion(output.view(sequence_length,input_size),labels)\n",
    "        loss.append(loss_temp.data)\n",
    "        loss_temp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        vals,max_indxs = (output.data).max(2)\n",
    "        acc_temp = (max_indxs==(labels.data)).sum()\n",
    "        accuracy.append(acc_temp/sequence_length)\n",
    "        #print(loss)\n",
    "        #print(accuracy)\n",
    "    print(epoch)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for this later \n",
    "primer = \"<start>\\n\"\n",
    "start_token = []\n",
    "for chars in primer:\n",
    "    start_token.append(ord(chars))\n",
    "    \n",
    "start_token = np.eye(128)[start_token]\n",
    "start_token = torch.from_numpy(start_token).float()\n",
    "start_token = autograd.Variable(start_token.view(1,8,input_size))\n",
    "#print(start_token.argmax(1))\n",
    "\n",
    "end = \"<end>\"\n",
    "end_token = []\n",
    "for chars in end:\n",
    "    end_token.append(ord(chars))\n",
    "    \n",
    "end_token = np.eye(128)[end_token]\n",
    "\n",
    "done = 1\n",
    "\n",
    "#Generating music\n",
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "chars = []\n",
    "\n",
    "#Each loop generates 1 char\n",
    "for i in range(100):\n",
    "    output,states = lstm(start_token,states)\n",
    "    \n",
    "    h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "    c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "    states = (h_0,c_0)\n",
    "    \n",
    "    #Currently I am taking the max of the hidden state as the letter that it\n",
    "    #produces, the TA on piazza said that it is not ideal to take max,\n",
    "    #can try implementing what he says later\n",
    "    val,indx = states[0].data.max(2)\n",
    "    chars.append(indx)\n",
    "    \n",
    "    start_token = torch.from_numpy(np.eye(128)[indx]).float()\n",
    "    start_token = autograd.Variable(start_token.view(1,1,input_size))\n",
    "    \n",
    "    #Need to implement a check for \"<end>\", generation should stop once\n",
    "    #\"<end>\" is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting back to strings\n",
    "outputs = []\n",
    "for letter in chars:\n",
    "    outputs.append(chr(letter))\n",
    "\n",
    "print(chars)\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
