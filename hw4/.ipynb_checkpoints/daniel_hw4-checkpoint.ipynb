{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Misc imports\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert string to one hot encoding\n",
    "def to_onehot(data):\n",
    "    ascii_list = []\n",
    "    elements_per_song = []\n",
    "    for songs in data:\n",
    "        elements_per_song.append(len(songs))\n",
    "        for chars in songs:\n",
    "            ascii_list.append(ord(chars))\n",
    "    onehot_vals = np.eye(128)[ascii_list]\n",
    "    return onehot_vals, np.array(ascii_list), elements_per_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is a list of list of chars, each string list is a song\n",
    "#onehot_ascii output are shifted up by 1, need to figure out what to do with 0 array labels\n",
    "def onehot_songs(data, sequence_length):\n",
    "    max_num_chars = 0\n",
    "    temp_list = []\n",
    "    onehot_ascii = []\n",
    "    onehot_songs = []\n",
    "    \n",
    "    #List of songs in ascii format\n",
    "    for song in data:\n",
    "        if len(song)>max_num_chars:\n",
    "            max_num_chars = len(song) #Finds longest song length\n",
    "        for chars in song:\n",
    "            temp_list.append(ord(chars))\n",
    "        onehot_ascii.append(temp_list)\n",
    "        temp_list = []\n",
    "        \n",
    "    #To make divisible by sequence_length per batch\n",
    "    max_num_chars = sequence_length-(max_num_chars%sequence_length)+max_num_chars\n",
    "    codding = np.append(np.zeros((1,128)),np.eye(128),0)    \n",
    "    \n",
    "    for i, ascii_song in enumerate(onehot_ascii):\n",
    "        ascii_song = np.array(ascii_song)+1 #since making first row of eye matrix all 0s\n",
    "        needed_0s = max_num_chars-len(ascii_song)\n",
    "        onehot_ascii[i] = np.pad(ascii_song,(0,needed_0s),'constant',constant_values=0)\n",
    "        onehot_songs.append(codding[onehot_ascii[i]])\n",
    "        \n",
    "    return onehot_ascii, onehot_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Data/input.txt\")\n",
    "text = file.readlines()\n",
    "file.close()\n",
    "\n",
    "text_array = np.asarray(text)\n",
    "\n",
    "#Creates list of each song\n",
    "indicies = np.where(text_array == '<start>\\n') #Location of where each abc file starts\n",
    "data = []\n",
    "for i in range(len(indicies[0])):\n",
    "    if i+1 == len(indicies[0]):\n",
    "        #For the last abc file\n",
    "        abc = text_array[indicies[0][i]:]\n",
    "    else:\n",
    "        abc = text_array[indicies[0][i]:indicies[0][i+1]]\n",
    "    data.append(''.join(abc))\n",
    "\n",
    "#print(data[0])\n",
    "#print(data[1123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "225\n",
      "[   0    1    2 ... 1121 1122 1123]\n",
      "[ 752  893 1050 ...  835  559  684]\n",
      "899\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "#80 - 20 split on data -> training and validation\n",
    "#Constants\n",
    "train_len = int(np.floor(len(data)*0.8))\n",
    "validation_len = len(data) - train_len\n",
    "\n",
    "print(train_len)\n",
    "print(validation_len)\n",
    "\n",
    "np.random.seed(0)\n",
    "#Each index references to a single song\n",
    "indxs = np.asarray(range(len(data)))\n",
    "print(indxs)\n",
    "np.random.shuffle(indxs)\n",
    "print(indxs)\n",
    "\n",
    "train_data = (np.asarray(data))[indxs[0:train_len]]\n",
    "validation_data = (np.asarray(data))[indxs[train_len:]]\n",
    "print(len(train_data))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97 98 99]\n",
      "<class 'numpy.ndarray'>\n",
      "[97 98 99]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Visualizing to_onehot\n",
    "b=5\n",
    "a,b = to_onehot('abc')\n",
    "print(np.argmax(a,1))\n",
    "print(type(np.argmax(a,1)))\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 61 116 117  98 115 117  63  11  89  59  52  50  11  85  59  88 115 102\n",
      " 111  40 116  33  81 112 109 108  98  45  33  85 105 102  11  85  59  68\n",
      " 118 106 109  33  66 112 101 105  98  33  81 112 109 108  98  45  33  85\n",
      " 105 102  11  83  59 113 112 109 108  98  11  91  59 106 101  59 105 111\n",
      "  46 113 112 109 108  98  46  52  50  11  78  59  51  48  53  11  77  59\n",
      "  50  48  57  11  76  59  66  11  66 100  33  70  71 125  66  63  66  33\n",
      "  66 103 125 102 100  33  67  66 125 102  48 103  48 102  48 100  48  33\n",
      "  67 100  48  67  48 125  66 100  33  70  71 125  66  63  66  33  66 103\n",
      " 125 102 100  33  67  66  48  67  48 125  50  33 100  66  33  66  70  59\n",
      " 125  51  33 100  66  33  66  67 125 125  11 125  59 100 102  33 100 102\n",
      " 125  98  51  33  98 103 125 102 100  33  67  66 125 102  48 103  48 102\n",
      "  48 100  48  33  67  66 125 100 102  33 100 102 125  98  51  33  98 103\n",
      " 125 102 100  33  67  66  48  67  48 125  50  33 100  66  33  66  67  59\n",
      " 125  51  33 100  66  33  66  70 125 125  11  61 102 111 101  63  11   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "375\n",
      "[ 61 116 117  98 115 117  63  11  89  59  56  51  11  85  59  69 106 111\n",
      " 104 109 102  33  83 102 102 109  45  33  85 105 102  11  83  59 115 102\n",
      " 102 109  11  84  59  76 102 119 106 111  33  67 118 115 108 102  45  33\n",
      " 120 105 112  33 104 112 117  33 106 117  33 103 115 112 110  33 117 105\n",
      " 102  33  78 118 115 113 105 122  33 103  98 110 106 109 122  47  11  69\n",
      "  59  76 102 119 106 111  33  67 118 115 108 102  59  33  86 113  33  68\n",
      " 109 112 116 102  47  11  91  59 106 101  59 105 111  46 115 102 102 109\n",
      "  46  56  51  11  78  59  68 125  11  76  59  69  11  69  51  69  71  33\n",
      "  70  69  67  45  66  45 125 127  69  52  71  33  66 101 127 101  51 125\n",
      "  67 102 102 103  33 102  51 101 102 125 103  51 102 101  33  67  66  71\n",
      "  70 125  11  69  51  69  71  33  70  69  67  45  66  45 125 127  69  52\n",
      "  71  33  66 101 127 101  51 125  67 102 102 103  33 102  51 101  67 125\n",
      "  50  33  66  71  70  71  33  69  51  71  70  59 125  51  33  66  71  70\n",
      "  71  33  69  53 125 125  11 125  59  66  67 101 102  33 127 103  52  98\n",
      " 125  98 103 127 103  51  33 102  67 127  67  51 125  66  67 101 102  33\n",
      " 127 103  52  98 125  98 103 127 103  51  33 102  51 101  67 125  11  66\n",
      "  67 101 102  33 127 103  52  98 125  98 103 127 103  51  33 102  67 127\n",
      "  67  51 125 101  51 101  67  33  66  67 101  67 125  50  33  66  71  70\n",
      "  71  33  69  53  59 125  51  33  66  71  70  71  33  69  51  71  70 125\n",
      " 125  11  61 102 111 101  63  11   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "#Visualizing onehot_songs\n",
    "train_data[0][:]\n",
    "#len(train_data[0:2])\n",
    "a,b = onehot_songs(train_data[0:2],25)\n",
    "print(b[0][374])\n",
    "print(a[0])\n",
    "print(len(a[0]))\n",
    "print(a[1])\n",
    "print(len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM constants\n",
    "input_size = 128 #num_classes = 128\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "batch_size = 1\n",
    "sequence_length = 25\n",
    "\n",
    "#Data\n",
    "#combines all the songs and makes a single string, converted to ascii and onehot\n",
    "onehot_train, train_labels, train_NumPerSong = to_onehot(train_data)\n",
    "onehot_validation, validation_labels, validation_NumPerSong = to_onehot(validation_data)\n",
    "#print(train_labels[0])\n",
    "#print(onehot_train[0])\n",
    "\n",
    "#or using onehot song encoddings\n",
    "#onehot_train, train_labels_padded = onehot_songs(train_data,sequence_length)\n",
    "#onehot_validation, validation_labels_padded = onehot_songs(validation_data,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927\n",
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(train_NumPerSong))\n",
    "print(np.min(train_NumPerSong))\n",
    "len(train_labels_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run first in single batches, each batch has 25 chars per(sequence length 25), just run it in order of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rnn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=1,batch_size=1,\n",
    "                 sequence_length=25,batch_first = True):\n",
    "        super(lstm_rnn, self).__init__()\n",
    "        #Constants\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        #Initializing model\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first = batch_first)\n",
    "        \n",
    "    def forward(self,data,initial):\n",
    "        #data = data.view(self.batch_size, self.sequence_length, self.input_size)\n",
    "        output,hidden = self.lstm(data,initial)\n",
    "        return output,hidden\n",
    "        \n",
    "    def init_hidden(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_hidden = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_hidden = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_hidden\n",
    "    \n",
    "    def init_cell(self,zero=1):\n",
    "        if zero == 0:\n",
    "            #Random initialization\n",
    "            initial_cell = autograd.Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        else:\n",
    "            #Zero initialization\n",
    "            initial_cell = autograd.Variable(torch.zeros(self.num_layers,self.batch_size,self.hidden_size))\n",
    "        return initial_cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm_rnn(input_size,hidden_size,num_layers,batch_size,\n",
    "                sequence_length,batch_first = batch_first)\n",
    "print(lstm.input_size)\n",
    "print(lstm.hidden_size)\n",
    "print(lstm.batch_size)\n",
    "print(lstm.sequence_length)\n",
    "print(lstm.num_layers)\n",
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates batch indicies, +1 since arange is [,), exclusive last element\n",
    "batch_indicies = np.arange(0,len(onehot_train)-sequence_length+1,sequence_length)\n",
    "print(len(batch_indicies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "loss = []\n",
    "accuracy = []\n",
    "\n",
    "#CAN ADD LINEAR LAYER AND BATCH PROCESSING \n",
    "for epoch in range(5):\n",
    "    for indx in batch_indicies:\n",
    "        optimizer.zero_grad()\n",
    "        temp_data = onehot_train[indx:indx+25]\n",
    "        temp_data = torch.from_numpy(temp_data).float()\n",
    "        temp_data = autograd.Variable(temp_data.view(batch_size,sequence_length,input_size))\n",
    "    \n",
    "        #output is of whole sequence, states hidden is same as last element of output, also\n",
    "        #has cell hidden state\n",
    "        output, states = lstm(temp_data,states)\n",
    "        \n",
    "        h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "        c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "        states = (h_0,c_0)\n",
    "        #hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        \n",
    "        #Add 1 since we are using the next letter as the label\n",
    "        labels = autograd.Variable(torch.LongTensor(train_labels[indx+1:indx+25+1]))\n",
    "        \n",
    "        #Later try adding a linear layer here\n",
    "        \n",
    "        loss_temp = criterion(output.view(sequence_length,input_size),labels)\n",
    "        loss.append(loss_temp.data)\n",
    "        loss_temp.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        vals,max_indxs = (output.data).max(2)\n",
    "        acc_temp = (max_indxs==(labels.data)).sum()\n",
    "        accuracy.append(acc_temp/sequence_length)\n",
    "        #print(loss)\n",
    "        #print(accuracy)\n",
    "    print(epoch)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function for this later \n",
    "primer = \"<start>\\n\"\n",
    "start_token = []\n",
    "for chars in primer:\n",
    "    start_token.append(ord(chars))\n",
    "    \n",
    "start_token = np.eye(128)[start_token]\n",
    "start_token = torch.from_numpy(start_token).float()\n",
    "start_token = autograd.Variable(start_token.view(1,8,input_size))\n",
    "#print(start_token.argmax(1))\n",
    "\n",
    "end = \"<end>\"\n",
    "end_token = []\n",
    "for chars in end:\n",
    "    end_token.append(ord(chars))\n",
    "    \n",
    "end_token = np.eye(128)[end_token]\n",
    "\n",
    "done = 1\n",
    "\n",
    "#Generating music\n",
    "h_0 = lstm.init_hidden()\n",
    "c_0 = lstm.init_cell()\n",
    "states = (h_0,c_0)\n",
    "chars = []\n",
    "\n",
    "#Each loop generates 1 char\n",
    "for i in range(100):\n",
    "    output,states = lstm(start_token,states)\n",
    "    \n",
    "    h_0 = autograd.Variable(states[0].data, requires_grad=True)\n",
    "    c_0 = autograd.Variable(states[1].data, requires_grad=True)\n",
    "    states = (h_0,c_0)\n",
    "    \n",
    "    #Currently I am taking the max of the hidden state as the letter that it\n",
    "    #produces, the TA on piazza said that it is not ideal to take max,\n",
    "    #can try implementing what he says later\n",
    "    val,indx = states[0].data.max(2)\n",
    "    chars.append(indx)\n",
    "    \n",
    "    start_token = torch.from_numpy(np.eye(128)[indx]).float()\n",
    "    start_token = autograd.Variable(start_token.view(1,1,input_size))\n",
    "    \n",
    "    #Need to implement a check for \"<end>\", generation should stop once\n",
    "    #\"<end>\" is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting back to strings\n",
    "outputs = []\n",
    "for letter in chars:\n",
    "    outputs.append(chr(letter))\n",
    "\n",
    "print(chars)\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
